{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tests import generate_regression_data, test_regression_model, test_knn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Необходимо реализовать класс для решения задачи восстановления регрессии методом ближайших соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для класса `KnnKdtreeRegressor` обучение заключается в построении k-d дерева по обучающей выборке. K-d дерево является бинарным деревом, которое соответствует разделению пространства, где находятся элементы обучающей выборки, гиперплоскостями, ортогональными осям координат. Каждой вершине k-d дерева соответствует гиперпрямоугольник в этом пространстве. Корневой вершине соответствует гиперпрямоугольник, содержащий все точки обучающей выборки.\n",
    "\n",
    "Построение k-d дерева реализовано на основе алгоритма ID3 для построения дерева принятия решений. Ось, по которой будет проводиться разбиение в каждой вершине выбирается случайным образом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnnKdtreeRegressor(object):\n",
    "    '''Регрессор реализует взвешенное усреднение по ближайшим соседям.\n",
    "    При подсчете расстояния используется l1-метрика.\n",
    "    Поиск ближайшего соседа осуществляется поиском по kd-дереву.\n",
    "    Параметры\n",
    "    ----------\n",
    "    n_neighbors : int, optional\n",
    "        Число ближайших соседей, учитывающихся в усреднении\n",
    "    weights : str, optional (default = 'uniform')\n",
    "        веса, используемые в голосовании. Возможные значения:\n",
    "        - 'uniform' : все веса равны.\n",
    "        - 'distance' : веса обратно пропорциональны расстоянию до классифицируемого объекта\n",
    "        -  функция, которая получает на вход массив расстояний и возвращает массив весов\n",
    "    leaf_size: int, optional\n",
    "        Максимально допустимый размер листа дерева\n",
    "    '''\n",
    "    def __init__(self, n_neighbors=1, weights='uniform', leaf_size=30):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.leaf_size = leaf_size\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        '''Обучение модели - построение kd-дерева\n",
    "        Параметры\n",
    "        ----------\n",
    "        x : двумерный массив признаков размера n_queries x n_features\n",
    "        y : массив/список правильных меток размера n_queries\n",
    "        Выход\n",
    "        -------\n",
    "        Метод возвращает обученную модель\n",
    "        '''\n",
    "        def build_kdtree(indices, parent=None):\n",
    "            if len(indices) <= self.leaf_size:\n",
    "                return Leaf(parent, indices)\n",
    "            axis = np.random.randint(x.shape[1])  # splitting axis\n",
    "            median = np.median(self.x[indices, axis])\n",
    "            left_indices = indices[np.argwhere(x[indices, axis] < median).flatten()]\n",
    "            right_indices = indices[np.argwhere(x[indices, axis] >= median).flatten()]\n",
    "            if len(left_indices) == 0 or len(right_indices) == 0:\n",
    "                return Leaf(parent, indices)\n",
    "            node = Node(parent, axis, median)\n",
    "            node.left = build_kdtree(left_indices, parent=node)\n",
    "            node.right = build_kdtree(right_indices, parent=node)\n",
    "            return node\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.tree = build_kdtree(indices=np.arange(x.shape[0]))\n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" Восстановление регрессии для входных объектов - поиск по метрическому дереву ближайших соседей\n",
    "        и взвешенное усреднение\n",
    "        Параметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_queries x n_features\n",
    "        Выход\n",
    "        -------\n",
    "        y : Массив размера n_queries\n",
    "        \"\"\"\n",
    "        def get_weights(distances, weights):\n",
    "            if weights == 'uniform':\n",
    "                return None\n",
    "            elif weights == 'distance':\n",
    "                for i, dist in enumerate(distances):\n",
    "                    if 0. in dist:\n",
    "                        distances[i] = dist == 0.\n",
    "                    else:\n",
    "                        distances[i] = 1. / dist\n",
    "                return distances\n",
    "            elif callable(weights):\n",
    "                return weights(distances)\n",
    "\n",
    "        neigh_dist, neigh_indarray = self.kneighbors(x, self.n_neighbors)\n",
    "        weights = get_weights(neigh_dist, self.weights)\n",
    "        if weights is None:\n",
    "            y = np.mean(self.y[neigh_indarray], axis=1)\n",
    "        else:\n",
    "            y = np.sum(self.y[neigh_indarray] * weights, axis=1) / np.sum(weights, axis=1)  # weighted arithmetic mean\n",
    "        return y\n",
    "\n",
    "    def kneighbors(self, x, n_neighbors):\n",
    "        \"\"\"Возвращает n_neighbors ближайших соседей для всех входных объектов при помощи поска по kd-дереву\n",
    "        и расстояния до них\n",
    "        Параметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_queries x n_features\n",
    "        Выход\n",
    "        -------\n",
    "        neigh_dist массив размера n_queries х n_neighbors\n",
    "        расстояния до ближайших элементов\n",
    "        neigh_indarray, массив размера n_queries x n_neighbors\n",
    "        индексы ближайших элементов\n",
    "        \"\"\"\n",
    "        def distance(p, q):\n",
    "            return np.sum(abs(p - q))  # l1 distance, also known as taxicab metric\n",
    "\n",
    "        def get_leaf(point):\n",
    "            node = self.tree\n",
    "            while isinstance(node, Node):\n",
    "                node = node.left if point[node.axis] < node.median else node.right\n",
    "            return node\n",
    "\n",
    "        def get_leaf_neighbors(point, leaf):\n",
    "            distances = np.array(list(map(distance, self.x[leaf.indices], np.ones((leaf.size, len(point))) * point)))\n",
    "            sort_by_distances = distances.argsort()\n",
    "            distances, indices = distances[sort_by_distances], leaf.indices[sort_by_distances]\n",
    "            if len(distances) > n_neighbors:\n",
    "                distances, indices = distances[:n_neighbors], indices[:n_neighbors]\n",
    "            return distances, indices\n",
    "\n",
    "        neigh_dist, neigh_indarray = np.empty((x.shape[0], n_neighbors)), np.empty((x.shape[0], n_neighbors), dtype=int)\n",
    "        distances, indices, furthest_distance = None, None, np.inf\n",
    "\n",
    "        def update_neighbors(point, node):\n",
    "            nonlocal distances, indices, furthest_distance\n",
    "            if isinstance(node, Leaf):\n",
    "                new_distances, new_indices = get_leaf_neighbors(point, node)\n",
    "                distances, indices = np.append(distances, new_distances), np.append(indices, new_indices)\n",
    "                sort_by_distances = distances.argsort()\n",
    "                distances, indices = distances[sort_by_distances], indices[sort_by_distances]\n",
    "                if len(distances) > n_neighbors:\n",
    "                    distances, indices = distances[:n_neighbors], indices[:n_neighbors]\n",
    "                if len(distances) == n_neighbors:\n",
    "                    furthest_distance = distances[-1]\n",
    "            elif distance(point[node.axis], node.median) < furthest_distance:\n",
    "                update_neighbors(point, node.left)\n",
    "                update_neighbors(point, node.right)\n",
    "            else:\n",
    "                next_branch = node.left if point[node.axis] < node.median else node.right\n",
    "                update_neighbors(point, next_branch)\n",
    "\n",
    "\n",
    "        for i, point in enumerate(x):\n",
    "            leaf = get_leaf(point)\n",
    "            distances, indices = get_leaf_neighbors(point, leaf)\n",
    "            if len(distances) == n_neighbors:\n",
    "                furthest_distance = distances[-1]\n",
    "            node = leaf.parent\n",
    "            while node is not None:\n",
    "                if distance(point[node.axis], node.median) < furthest_distance:\n",
    "                    opposite_branch = node.left if point[node.axis] >= node.median else node.right\n",
    "                    update_neighbors(point, opposite_branch)\n",
    "                node = node.parent\n",
    "            neigh_dist[i], neigh_indarray[i] = distances, indices\n",
    "            furthest_distance = np.inf\n",
    "        return neigh_dist, neigh_indarray\n",
    "\n",
    "    def score(self, y_gt, y_pred):\n",
    "        \"\"\"Возвращает среднеквадратичную ошибку восстановления регрессии\n",
    "        Параметры\n",
    "        ----------\n",
    "        y_gt : правильные метки объектов\n",
    "        y_pred: предсказанные метки объектов\n",
    "        Выход\n",
    "        -------\n",
    "        mse - среднеквадратичная ошибка восстановления регрессии\n",
    "        \"\"\"\n",
    "        y_gt, y_pred = np.array(y_gt), np.array(y_pred)\n",
    "        return 1 / len(y_gt) * sum((y_gt - y_pred) ** 2)  # mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для хранения внутренних вершин k-d дерева. Конструктор принимает родителя `parent` вершины, номер `axis` оси пространства, по которой происходит разделение в данной вершине, значение медианы `median` обучающей выборки по оси `axis`, а также левую и правую дочерние вершины `left` и `right`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, parent, axis, median, left=None, right=None):\n",
    "        self.parent = parent\n",
    "        self.axis = axis\n",
    "        self.median = median\n",
    "        self.left = left\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для хранения листьев k-d дерева. Конструктор принимает родителя `parent` листа и индексы `indices` элементов обучающей выборки, которые были распределены в данный лист. Также определено свойство `size`, возврашающее количество элементов обучающей выборки, попавших в данный лист."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaf:\n",
    "    def __init__(self, parent, indices):\n",
    "        self.parent = parent\n",
    "        self.indices = indices\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your mse is 0.014037991922886342\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "n_features = 10\n",
    "n_train = 1000\n",
    "n_test = 1500\n",
    "W = np.random.rand(n_features + 1)\n",
    "trainX = np.append(np.random.rand(n_train, n_features), np.ones((n_train, 1)), axis=1)\n",
    "trainY = trainX @ W\n",
    "testX = np.append(np.random.rand(n_test, n_features), np.ones((n_test, 1)), axis=1)\n",
    "testY = testX @ W\n",
    "n_neighbors = 15\n",
    "weights = 'distance'\n",
    "leaf_size = 100\n",
    "knn = KnnKdtreeRegressor(n_neighbors, weights, leaf_size)\n",
    "knn.fit(trainX, trainY)\n",
    "y_pred = knn.predict(testX)\n",
    "mse = knn.score(testY, y_pred)\n",
    "print('Your mse is %s' % str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Покройте ваш класс тестами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импортированный тест метода ближайших соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_predict test passed\n",
      "score test passed\n",
      "[[0]] [[0]]\n",
      "kneighbors test passed\n",
      "accuracy test passed\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "n_neighbors = 5\n",
    "weights = 'uniform'\n",
    "leaf_size = 10\n",
    "model = KnnKdtreeRegressor(n_neighbors, weights, leaf_size)\n",
    "test_knn_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тест на корректость работы метода ближайших соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестирование корректности работы класса `KnnKdtreeRegressor` осуществляется путем сравнения полученных результатов с результатами работы класса `KNeighborsRegressor`, реализованного в библиотеке машинного обучения `sklearn`. В конструктор класса `KNeighborsRegressor` подаются те же параметры, что и в `KnnKdtreeRegressor`, а также параметр `algorithm='kd_tree'`, определяющий алгоритм, используемый для вычисления ближайших соседей и `p=1` ‒ параметр мощности для метрики Минковского, при `p=1`, эта метрика эквивалентна манхэттенской метрике, используемой в `KnnKdtreeRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mtest passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def create_models(n_neighbors=15, weights='uniform', leaf_size=100):\n",
    "    model = KnnKdtreeRegressor(n_neighbors, weights, leaf_size)\n",
    "    sklearn_model = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, leaf_size=leaf_size,\n",
    "                                        algorithm='kd_tree', p=1)\n",
    "    return model, sklearn_model\n",
    "\n",
    "\n",
    "def generate_data(n_features=10, n_train=1000, n_test=1500):\n",
    "    W = np.random.rand(n_features + 1)\n",
    "    trainX = np.c_[np.random.rand(n_train, n_features), np.ones(n_train)]\n",
    "    trainY = trainX @ W\n",
    "    testX = np.c_[np.random.rand(n_test, n_features), np.ones(n_test)]\n",
    "    testY = testX @ W\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "def fit_models(model, sklearn_model, trainX, trainY):\n",
    "    model.fit(trainX, trainY)\n",
    "    sklearn_model.fit(trainX, trainY)\n",
    "\n",
    "\n",
    "epsilon = 1e-10\n",
    "\n",
    "\n",
    "class Color:\n",
    "    green = '\\033[32m'\n",
    "    red = '\\033[91m'\n",
    "    end = '\\033[0m'\n",
    "\n",
    "\n",
    "def test_score(model, n=100):\n",
    "    y_gt, y_pred = np.random.rand(n), np.random.rand(n)\n",
    "    assert abs(model.score(y_gt, y_pred) - mean_squared_error(y_gt, y_pred)) < epsilon, 'wrong score function'\n",
    "\n",
    "\n",
    "def test_kneighbors(model, sklearn_model, testX):\n",
    "    neigh_dist, neigh_indarray = model.kneighbors(testX, model.n_neighbors)\n",
    "    sklearn_neigh_dist, sklearn_neigh_indarray = sklearn_model.kneighbors(testX, model.n_neighbors)\n",
    "    assert np.all(neigh_indarray == sklearn_neigh_indarray) and \\\n",
    "           np.all(abs(neigh_dist - sklearn_neigh_dist) < epsilon), 'wrong neighbors'\n",
    "\n",
    "\n",
    "def test_predict(model, sklearn_model, testX, testY):\n",
    "    y_pred = model.predict(testX)\n",
    "    sklearn_y_pred = sklearn_model.predict(testX)\n",
    "    assert abs(model.score(testY, y_pred) - mean_squared_error(testY, sklearn_y_pred)) < epsilon, \\\n",
    "        'unacceptably low prediction accuracy'\n",
    "\n",
    "\n",
    "def test(n_neighbors=1, weights='uniform', leaf_size=30, n_features=1, n_train=150, n_test=150, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    model, sklearn_model = create_models(n_neighbors, weights, leaf_size)\n",
    "    trainX, trainY, testX, testY = generate_data(n_features, n_train, n_test)\n",
    "    fit_models(model, sklearn_model, trainX, trainY)\n",
    "    parameters = {\n",
    "        'seed': seed,\n",
    "        'n_neighbors': n_neighbors,\n",
    "        'weights': weights,\n",
    "        'leaf_size': leaf_size,\n",
    "        'n_features': n_features,\n",
    "        'n_train': n_train,\n",
    "        'n_test': n_test\n",
    "    }\n",
    "    try:\n",
    "        test_score(model)\n",
    "        test_kneighbors(model, sklearn_model, testX)\n",
    "        test_predict(model, sklearn_model, testX, testY)\n",
    "    except AssertionError as error:\n",
    "        print(f'{Color.red}{error}{Color.end}')\n",
    "        test_passed = False\n",
    "    else:\n",
    "        test_passed = True\n",
    "    return test_passed, parameters\n",
    "\n",
    "\n",
    "test_passed, parameters = test(n_neighbors=15, weights='distance', leaf_size=50, n_features=10, n_train=500, n_test=500)\n",
    "if test_passed:\n",
    "    print(f'{Color.green}test passed{Color.end}')\n",
    "else:\n",
    "    for parameter, value in parameters.items():\n",
    "        print(parameter, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Примените ваш класс к самостоятельно подготовленной задаче восстановления регрессии и подберите оптимальные параметры для вашей модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбрана задача восстановления регрессии для данных с параметрами `n_features=10, n_train=100, n_test=100`, сгенерированных случайно при `np.random.seed(0)`. Оптимальные параметры `leaf_size, n_neighbors, weights` для данной задачи подбираются путем перебора всех допустимых значений этих параметров и выбора того набора параметров, при котором значение `mse` среднеквадратичной ошибки восстановления регрессии минимально. Для `leaf_size` и `n_neighbors` допустимы целые положительные значения от `1` до `n_train`. Параметр `weights` выбирается из двух вариантов: `uniform` и `distance`.\n",
    "\n",
    "Подобранные для данной задачи оптимальные параметры: `leaf_size=1, n_neighbors=5, weights='distance'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal leaf_size 1\n",
      "optimal n_neighbors 5\n",
      "optimal weights distance\n",
      "minimal mse 0.10439044483706239\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n_features = 10\n",
    "n_train = 100\n",
    "n_test = 100\n",
    "W = np.random.rand(n_features + 1)\n",
    "trainX = np.c_[np.random.rand(n_train, n_features), np.ones(n_train)]\n",
    "trainY = trainX @ W\n",
    "testX = np.c_[np.random.rand(n_test, n_features), np.ones(n_test)]\n",
    "testY = testX @ W\n",
    "\n",
    "\n",
    "def get_optimal_parameters(weights):\n",
    "    parameters_mse = list()\n",
    "    for leaf_size in range(1, n_train + 1):\n",
    "        for n_neighbors in range(1, n_train + 1):\n",
    "            knn = KnnKdtreeRegressor(n_neighbors, weights, leaf_size)\n",
    "            knn.fit(trainX, trainY)\n",
    "            y_pred = knn.predict(testX)\n",
    "            mse = knn.score(testY, y_pred)\n",
    "            parameters_mse.append([leaf_size, n_neighbors, mse])\n",
    "    parameters_mse = np.array(parameters_mse)\n",
    "    return parameters_mse[np.argmin(parameters_mse[:, 2])]\n",
    "\n",
    "\n",
    "parameters_uniform = get_optimal_parameters('uniform')\n",
    "parameters_distance = get_optimal_parameters('distance')\n",
    "if parameters_uniform[2] < parameters_distance[2]:\n",
    "    optimal_parameters = parameters_uniform\n",
    "    weights = 'uniform'\n",
    "else:\n",
    "    optimal_parameters = parameters_distance\n",
    "    weights = 'distance'\n",
    "\n",
    "\n",
    "print('optimal leaf_size', int(optimal_parameters[0]))\n",
    "print('optimal n_neighbors', int(optimal_parameters[1]))\n",
    "print('optimal weights', weights)\n",
    "print('minimal mse', optimal_parameters[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "%% md\n",
     "\n",
     "# Восстановление регрессии методом ближайших соседей\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
